{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 能動学習 (Active Learning) デモ — 画像分類編 (手書き数字)\n",
    "\n",
    "## 概要\n",
    "**能動学習**とは、モデル自身が「次にどのデータにラベルを付けるべきか」を選ぶ手法です。\n",
    "\n",
    "本ノートブックでは、手書き数字画像の分類タスクを題材に、能動学習の効果を体験します。\n",
    "回帰版デモ (`active_learning_demo.ipynb`) と対比しながら、**分類問題における能動学習**の仕組みを理解しましょう。\n",
    "\n",
    "データセットには scikit-learn 組み込みの `load_digits`（8×8ピクセル、0〜9の手書き数字 1797枚）を使用します。\n",
    "\n",
    "### 回帰版との違い\n",
    "| | 回帰版 | 分類版 (本デモ) |\n",
    "|---|---|---|\n",
    "| データ | 2D連続関数 | 手書き数字画像 (8×8) |\n",
    "| モデル | ガウス過程回帰 (GPR) | ロジスティック回帰 |\n",
    "| 不確実性の指標 | 予測標準偏差 σ(x) | 予測エントロピー H(x) |\n",
    "| 評価指標 | RMSE | Accuracy |\n",
    "\n",
    "### このデモで体験すること\n",
    "1. 少数の初期ラベル付きデータで分類モデルを構築\n",
    "2. 能動学習ループ：予測が不確実なサンプルを逐次追加\n",
    "3. ランダムサンプリングとの性能比較\n",
    "\n",
    "### 用語\n",
    "| 用語 | 説明 |\n",
    "|------|------|\n",
    "| ラベルなしプール | まだラベルを取得していない画像の集合 |\n",
    "| エントロピー | 予測確率分布の不確実性を表す指標（高いほど不確実） |\n",
    "| 獲得関数 | 次にどのサンプルにラベルを付けるべきかを決める基準 |\n",
    "| マージンサンプリング | 上位2クラスの予測確率の差が小さいサンプルを選ぶ戦略 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = np.random.default_rng(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 手書き数字データの読み込みと確認\n",
    "\n",
    "scikit-learn 組み込みの `load_digits` データセットを使用します。\n",
    "0〜9の手書き数字画像（8×8ピクセル、グレースケール）が 1797枚含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手書き数字データの読み込み\n",
    "digits = load_digits()\n",
    "X_all, y_all = digits.data, digits.target\n",
    "\n",
    "# テストセットを分離\n",
    "X_work, X_test, y_work, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=RANDOM_STATE, stratify=y_all\n",
    ")\n",
    "\n",
    "# 特徴量のスケーリング\n",
    "scaler = StandardScaler()\n",
    "X_work = scaler.fit_transform(X_work)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "IMG_SIZE = 8  # 画像の幅・高さ\n",
    "\n",
    "print(f\"全データ:     {len(X_all)} 枚\")\n",
    "print(f\"作業用データ: {X_work.shape[0]} 枚\")\n",
    "print(f\"テストデータ: {X_test.shape[0]} 枚\")\n",
    "print(f\"画像サイズ:   {IMG_SIZE}×{IMG_SIZE} = {X_work.shape[1]} ピクセル\")\n",
    "print(f\"クラス数:     {len(np.unique(y_work))} (0〜9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像の表示\n",
    "fig, axes = plt.subplots(2, 10, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.where(y_work == i % 10)[0][i // 10]\n",
    "    img = scaler.inverse_transform(X_work[idx:idx+1]).reshape(IMG_SIZE, IMG_SIZE)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(str(y_work[idx]), fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Digits Sample Images\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 初期ラベル付きデータの準備\n",
    "\n",
    "能動学習のスタート地点として、少数のラベル付きデータを用意します。\n",
    "残りは「ラベルなしプール」として、能動学習で逐次ラベルを取得していきます。\n",
    "\n",
    "（実際のアプリケーションでは、ラベル付けは人間のアノテーターが行います。\n",
    "ここでは元のラベルを「アノテーターに問い合わせた結果」として使います。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INITIAL = 50  # 初期ラベル付きデータ数（各クラス5枚）\n",
    "\n",
    "# 各クラスから均等にサンプリング\n",
    "initial_indices = []\n",
    "for digit in range(10):\n",
    "    class_indices = np.where(y_work == digit)[0]\n",
    "    chosen = rng.choice(class_indices, size=N_INITIAL // 10, replace=False)\n",
    "    initial_indices.extend(chosen)\n",
    "initial_indices = np.array(initial_indices)\n",
    "\n",
    "# 初期データとプールに分割\n",
    "pool_indices = np.setdiff1d(np.arange(len(X_work)), initial_indices)\n",
    "\n",
    "X_labeled = X_work[initial_indices]\n",
    "y_labeled = y_work[initial_indices]\n",
    "X_pool = X_work[pool_indices]\n",
    "y_pool = y_work[pool_indices]  # 「まだ見えない」ラベル\n",
    "\n",
    "print(f\"初期ラベル付きデータ: {len(X_labeled)} 枚\")\n",
    "print(f\"ラベルなしプール:     {len(X_pool)} 枚\")\n",
    "print(f\"テストデータ:         {len(X_test)} 枚\")\n",
    "print(f\"\\n初期データのクラス分布:\")\n",
    "for digit in range(10):\n",
    "    count = np.sum(y_labeled == digit)\n",
    "    print(f\"  {digit}: {count} 枚\", end=\"  \")\n",
    "    if digit == 4:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 分類モデルの構築と不確実性の計算\n",
    "\n",
    "分類問題では、予測確率から**不確実性**を計算できます。\n",
    "代表的な指標として以下の3つがあります：\n",
    "\n",
    "### 不確実性の指標\n",
    "\n",
    "1. **エントロピー (Entropy Sampling)**\n",
    "$$H(x) = -\\sum_{k=1}^{K} p_k(x) \\log p_k(x)$$\n",
    "予測確率分布が均一に近いほどエントロピーが高い（＝不確実）\n",
    "\n",
    "2. **マージン (Margin Sampling)**\n",
    "$$M(x) = p_{\\text{1st}}(x) - p_{\\text{2nd}}(x)$$\n",
    "上位2クラスの確率差が小さいほど不確実（マージンが小さいものを選ぶ）\n",
    "\n",
    "3. **最小確信度 (Least Confidence)**\n",
    "$$LC(x) = 1 - p_{\\text{max}}(x)$$\n",
    "最も確率の高いクラスの確率が低いほど不確実\n",
    "\n",
    "本デモでは**エントロピーサンプリング**を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(X_train, y_train):\n",
    "    \"\"\"ロジスティック回帰モデルを構築・学習する\"\"\"\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=500,\n",
    "        solver=\"lbfgs\",\n",
    "        multi_class=\"multinomial\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        C=1.0,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def compute_entropy(clf, X):\n",
    "    \"\"\"予測確率からエントロピーを計算する\"\"\"\n",
    "    proba = clf.predict_proba(X)\n",
    "    # log(0)を避けるためにクリップ\n",
    "    proba = np.clip(proba, 1e-10, 1.0)\n",
    "    entropy = -np.sum(proba * np.log(proba), axis=1)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, X_test, y_test):\n",
    "    \"\"\"テストセットでの精度を計算\"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期モデルの学習と評価\n",
    "clf_init = build_classifier(X_labeled, y_labeled)\n",
    "acc_init = evaluate_classifier(clf_init, X_test, y_test)\n",
    "print(f\"初期モデルの精度: {acc_init:.4f} ({len(X_labeled)} 枚で学習)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期モデルのエントロピー分布を可視化\n",
    "entropy_init = compute_entropy(clf_init, X_pool)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# エントロピーのヒストグラム\n",
    "ax = axes[0]\n",
    "ax.hist(entropy_init, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(np.mean(entropy_init), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(entropy_init):.2f}\")\n",
    "ax.set_xlabel(\"Entropy\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Entropy Distribution of Pool Samples\")\n",
    "ax.legend()\n",
    "\n",
    "# エントロピーが高いサンプル（モデルが迷っている画像）を表示\n",
    "ax = axes[1]\n",
    "top_k = 10\n",
    "top_indices = np.argsort(entropy_init)[-top_k:]\n",
    "for i, idx in enumerate(top_indices):\n",
    "    img = scaler.inverse_transform(X_pool[idx:idx+1]).reshape(IMG_SIZE, IMG_SIZE)\n",
    "    ax_sub = fig.add_axes([0.55 + (i % 5) * 0.09, 0.55 - (i // 5) * 0.45, 0.08, 0.35])\n",
    "    ax_sub.imshow(img, cmap=\"gray\")\n",
    "    pred = clf_init.predict(X_pool[idx:idx+1])[0]\n",
    "    true = y_pool[idx]\n",
    "    ax_sub.set_title(f\"P:{pred} T:{true}\", fontsize=7, color=\"red\" if pred != true else \"green\")\n",
    "    ax_sub.axis(\"off\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Most Uncertain Samples (P=Pred, T=True)\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 能動学習ループ\n",
    "\n",
    "### 獲得関数：エントロピーサンプリング\n",
    "\n",
    "候補プールの中から**予測エントロピーが最大のサンプル**を選んでラベルを付けます。\n",
    "\n",
    "```\n",
    "次のサンプル = argmax_{x ∈ プール} H(x)\n",
    "```\n",
    "\n",
    "直感的には「モデルがどの数字か最も迷っている画像」を優先的にラベル付けする、という戦略です。\n",
    "\n",
    "回帰版では GPR の予測標準偏差 σ(x) を使いましたが、分類版ではエントロピー H(x) が同様の役割を果たします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_loop(X_init, y_init, X_pool, y_pool,\n",
    "                         X_test, y_test, n_iterations=50, batch_size=10):\n",
    "    \"\"\"\n",
    "    能動学習ループ（エントロピーサンプリング）\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_init, y_init : 初期ラベル付きデータ\n",
    "    X_pool, y_pool : ラベルなしプール（y_poolは「問い合わせれば得られる」真のラベル）\n",
    "    X_test, y_test : テストデータ\n",
    "    n_iterations   : 能動学習の反復回数\n",
    "    batch_size     : 1回に追加するサンプル数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history : dict（精度履歴、選択されたサンプルなど）\n",
    "    \"\"\"\n",
    "    X_train = X_init.copy()\n",
    "    y_train = y_init.copy()\n",
    "    X_remaining = X_pool.copy()\n",
    "    y_remaining = y_pool.copy()\n",
    "\n",
    "    acc_history = []\n",
    "    n_samples_history = []\n",
    "    selected_images = []  # 選択された画像を保存\n",
    "    selected_entropies = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # 1. 分類器を学習\n",
    "        clf = build_classifier(X_train, y_train)\n",
    "        acc = evaluate_classifier(clf, X_test, y_test)\n",
    "        acc_history.append(acc)\n",
    "        n_samples_history.append(len(X_train))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Iteration {i:3d}: n={len(X_train):5d}, Accuracy={acc:.4f}\")\n",
    "\n",
    "        # 2. プールでのエントロピーを計算\n",
    "        entropy = compute_entropy(clf, X_remaining)\n",
    "\n",
    "        # 3. エントロピーが最大のサンプルを選択\n",
    "        idx = np.argsort(entropy)[-batch_size:]\n",
    "        selected_entropies.append(entropy[idx].mean())\n",
    "\n",
    "        # 4. 選択したサンプルを学習データに追加\n",
    "        if i < 5:  # 最初の数回は画像を保存\n",
    "            selected_images.append((X_remaining[idx].copy(), y_remaining[idx].copy()))\n",
    "        X_train = np.vstack([X_train, X_remaining[idx]])\n",
    "        y_train = np.concatenate([y_train, y_remaining[idx]])\n",
    "\n",
    "        # 5. プールから除去\n",
    "        X_remaining = np.delete(X_remaining, idx, axis=0)\n",
    "        y_remaining = np.delete(y_remaining, idx, axis=0)\n",
    "\n",
    "    # 最終モデルの評価\n",
    "    clf_final = build_classifier(X_train, y_train)\n",
    "    acc_final = evaluate_classifier(clf_final, X_test, y_test)\n",
    "    acc_history.append(acc_final)\n",
    "    n_samples_history.append(len(X_train))\n",
    "    print(f\"  Final:        n={len(X_train):5d}, Accuracy={acc_final:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc_history\": acc_history,\n",
    "        \"n_samples_history\": n_samples_history,\n",
    "        \"selected_images\": selected_images,\n",
    "        \"selected_entropies\": selected_entropies,\n",
    "        \"clf_final\": clf_final,\n",
    "        \"X_train_final\": X_train,\n",
    "        \"y_train_final\": y_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERATIONS = 50\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "print(f\"能動学習を実行中... ({N_ITERATIONS} iterations, batch_size={BATCH_SIZE})\")\n",
    "al_result = active_learning_loop(\n",
    "    X_labeled, y_labeled,\n",
    "    X_pool, y_pool,\n",
    "    X_test, y_test,\n",
    "    n_iterations=N_ITERATIONS,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "print(f\"\\nAccuracy: {al_result['acc_history'][0]:.4f} → {al_result['acc_history'][-1]:.4f}\")\n",
    "print(f\"学習データ: {N_INITIAL} → {al_result['n_samples_history'][-1]} 枚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 能動学習が選んだ画像の可視化\n",
    "\n",
    "各イテレーションで能動学習が選んだ画像を確認します。\n",
    "モデルが「判別に迷っている画像」が優先的に選ばれていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 能動学習が選んだ画像を表示（最初の5イテレーション）\n",
    "n_show = min(5, len(al_result[\"selected_images\"]))\n",
    "fig, axes = plt.subplots(n_show, BATCH_SIZE, figsize=(10, n_show * 1.5))\n",
    "\n",
    "for row in range(n_show):\n",
    "    X_sel, y_sel = al_result[\"selected_images\"][row]\n",
    "    for col in range(min(BATCH_SIZE, len(X_sel))):\n",
    "        ax = axes[row, col] if n_show > 1 else axes[col]\n",
    "        img = scaler.inverse_transform(X_sel[col:col+1]).reshape(IMG_SIZE, IMG_SIZE)\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"{y_sel[col]}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "    if n_show > 1:\n",
    "        axes[row, 0].set_ylabel(f\"Iter {row+1}\", fontsize=9, rotation=0, labelpad=30)\n",
    "\n",
    "plt.suptitle(\"Images Selected by Active Learning (per iteration)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ランダムサンプリングとの比較\n",
    "\n",
    "能動学習の効果を示すために、プールからランダムに画像を選ぶ戦略と比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_loop(X_init, y_init, X_pool, y_pool,\n",
    "                         X_test, y_test, n_iterations=50, batch_size=5, seed=0):\n",
    "    \"\"\"ランダムサンプリングによるベースライン\"\"\"\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    X_train = X_init.copy()\n",
    "    y_train = y_init.copy()\n",
    "    X_remaining = X_pool.copy()\n",
    "    y_remaining = y_pool.copy()\n",
    "\n",
    "    acc_history = []\n",
    "    n_samples_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        clf = build_classifier(X_train, y_train)\n",
    "        acc = evaluate_classifier(clf, X_test, y_test)\n",
    "        acc_history.append(acc)\n",
    "        n_samples_history.append(len(X_train))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Iteration {i:3d}: n={len(X_train):5d}, Accuracy={acc:.4f}\")\n",
    "\n",
    "        # ランダムに選択\n",
    "        idx = rng_local.choice(len(X_remaining), size=batch_size, replace=False)\n",
    "        X_train = np.vstack([X_train, X_remaining[idx]])\n",
    "        y_train = np.concatenate([y_train, y_remaining[idx]])\n",
    "        X_remaining = np.delete(X_remaining, idx, axis=0)\n",
    "        y_remaining = np.delete(y_remaining, idx, axis=0)\n",
    "\n",
    "    clf_final = build_classifier(X_train, y_train)\n",
    "    acc_final = evaluate_classifier(clf_final, X_test, y_test)\n",
    "    acc_history.append(acc_final)\n",
    "    n_samples_history.append(len(X_train))\n",
    "    print(f\"  Final:        n={len(X_train):5d}, Accuracy={acc_final:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc_history\": acc_history,\n",
    "        \"n_samples_history\": n_samples_history,\n",
    "        \"clf_final\": clf_final,\n",
    "        \"X_train_final\": X_train,\n",
    "        \"y_train_final\": y_train,\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"ランダムサンプリングを実行中... ({N_ITERATIONS} iterations, batch_size={BATCH_SIZE})\")\n",
    "rand_result = random_sampling_loop(\n",
    "    X_labeled, y_labeled,\n",
    "    X_pool, y_pool,\n",
    "    X_test, y_test,\n",
    "    n_iterations=N_ITERATIONS,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "print(f\"\\nAccuracy: {rand_result['acc_history'][0]:.4f} → {rand_result['acc_history'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 結果の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy曲線\n",
    "ax = axes[0]\n",
    "ax.plot(al_result[\"n_samples_history\"], al_result[\"acc_history\"],\n",
    "        \"o-\", label=\"Active Learning (Entropy)\", markersize=3)\n",
    "ax.plot(rand_result[\"n_samples_history\"], rand_result[\"acc_history\"],\n",
    "        \"s--\", label=\"Random Sampling\", markersize=3, alpha=0.7)\n",
    "ax.set_xlabel(\"Number of Labeled Samples\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_title(\"Learning Curve: Active Learning vs Random Sampling\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 差分プロット\n",
    "ax = axes[1]\n",
    "acc_diff = np.array(al_result[\"acc_history\"]) - np.array(rand_result[\"acc_history\"])\n",
    "ax.bar(al_result[\"n_samples_history\"], acc_diff, width=8, alpha=0.7,\n",
    "       color=[\"green\" if d > 0 else \"red\" for d in acc_diff])\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Number of Labeled Samples\")\n",
    "ax.set_ylabel(\"Accuracy Difference (AL - Random)\")\n",
    "ax.set_title(\"Active Learning Advantage\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"最終Accuracy比較 ({al_result['n_samples_history'][-1]} samples)\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Active Learning:  {al_result['acc_history'][-1]:.4f}\")\n",
    "print(f\"  Random Sampling:  {rand_result['acc_history'][-1]:.4f}\")\n",
    "diff = al_result['acc_history'][-1] - rand_result['acc_history'][-1]\n",
    "print(f\"  Difference:       {diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "for ax, (result, label) in zip(axes, [\n",
    "    (al_result, \"Active Learning\"),\n",
    "    (rand_result, \"Random Sampling\"),\n",
    "]):\n",
    "    clf = result[\"clf_final\"]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=range(10))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
    "    acc = result[\"acc_history\"][-1]\n",
    "    ax.set_title(f\"{label}\\n(Accuracy: {acc:.4f}, n={result['n_samples_history'][-1]})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エントロピーの推移（能動学習が選ぶサンプルの不確実性の変化）\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.plot(range(1, len(al_result[\"selected_entropies\"]) + 1),\n",
    "        al_result[\"selected_entropies\"], \"o-\", markersize=3)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Mean Entropy of Selected Samples\")\n",
    "ax.set_title(\"Entropy of Actively Selected Samples Over Iterations\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. クラスごとの分析\n",
    "\n",
    "能動学習がどのクラス（数字）を重点的にサンプリングしたかを分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各手法の最終学習データにおけるクラス分布\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for ax, (result, label) in zip(axes, [\n",
    "    (al_result, \"Active Learning\"),\n",
    "    (rand_result, \"Random Sampling\"),\n",
    "]):\n",
    "    counts = [np.sum(result[\"y_train_final\"] == d) for d in range(10)]\n",
    "    initial_counts = [np.sum(y_labeled == d) for d in range(10)]\n",
    "    added_counts = [c - ic for c, ic in zip(counts, initial_counts)]\n",
    "\n",
    "    x = np.arange(10)\n",
    "    ax.bar(x, initial_counts, label=\"Initial\", alpha=0.7)\n",
    "    ax.bar(x, added_counts, bottom=initial_counts, label=\"Added\", alpha=0.7)\n",
    "    ax.set_xlabel(\"Digit\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{label}: Class Distribution\")\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 能動学習で追加されたサンプルのクラス分布\n",
    "al_added = al_result[\"y_train_final\"][N_INITIAL:]\n",
    "print(\"能動学習で追加されたサンプルのクラス分布:\")\n",
    "for digit in range(10):\n",
    "    count = np.sum(al_added == digit)\n",
    "    bar = \"█\" * (count // 2)\n",
    "    print(f\"  {digit}: {count:3d} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. まとめ\n",
    "\n",
    "### 観察されたこと\n",
    "\n",
    "1. **学習曲線**: 能動学習はランダムサンプリングよりも少ないラベル付きデータで高い精度を達成\n",
    "2. **サンプル選択**: 能動学習は「判別が難しい画像」（崩れた字、似た数字の境界ケースなど）を優先的に選択\n",
    "3. **クラス分布**: 能動学習は識別が難しいクラス（例: 3と8、4と9）を多めにサンプリングする傾向\n",
    "4. **エントロピー推移**: イテレーションが進むにつれ、選択されるサンプルのエントロピーが低下（モデルの自信が向上）\n",
    "\n",
    "### 回帰版との対比\n",
    "\n",
    "| 観点 | 回帰版 (GPR) | 分類版 (本デモ) |\n",
    "|------|------|------|\n",
    "| 不確実性 | 予測標準偏差 σ(x) | 予測エントロピー H(x) |\n",
    "| 選択傾向 | データの空白地帯 → 複雑な領域 | 判別困難なサンプル → 境界ケース |\n",
    "| 性能指標 | RMSE の低下 | Accuracy の向上 |\n",
    "| 共通点 | **「モデルが最も不確実な場所を優先的に学習する」**という原理は同じ |\n",
    "\n",
    "### 能動学習の実応用\n",
    "\n",
    "- **医療画像**: 専門医によるアノテーションコストが高い → 能動学習で効率化\n",
    "- **自然言語処理**: テキスト分類のラベル付けコストを削減\n",
    "- **製造業**: 外観検査における不良品ラベルの効率的な収集\n",
    "\n",
    "### 発展的な話題\n",
    "\n",
    "- **Query-by-Committee**: 複数モデルの不一致度を不確実性として使用\n",
    "- **Bayesian Active Learning**: ベイズ推論ベースの不確実性推定\n",
    "- **Deep Active Learning**: 深層学習モデルでの能動学習（MC Dropout等）\n",
    "- **バッチモード能動学習**: 多様性を考慮した複数サンプルの同時選択\n",
    "\n",
    "### 補足: データセットについて\n",
    "本デモでは scikit-learn 組み込みの `load_digits`（8×8ピクセル、1797枚）を使用しました。\n",
    "より高解像度の MNIST（28×28ピクセル、70,000枚）でも同様の手法が適用可能です。\n",
    "データ規模が大きい場合、能動学習の効果（ラベル付けコスト削減）はさらに顕著になります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
